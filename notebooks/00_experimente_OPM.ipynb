{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"00experimenteOPM\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import gc\n",
    "import tracemalloc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import dill\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn import linear_model as sk_linear_model\n",
    "from sklearn import pipeline as sk_pipeline\n",
    "from sklearn import preprocessing as sk_preprocessing\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from pyspotstream.data.OPM import load_opm\n",
    "from pyspotstream.misc.drift_generator import generate_drift\n",
    "from pyspotstream.eval.evaluation import baseline_batch_experiment\n",
    "from pyspotstream.eval.evaluation import eval_multiple_instances\n",
    "from pyspotstream.plot.plotmulti import plot_multiple_instances_results\n",
    "from pyspotstream.eval.evaluation import eval_single_instances\n",
    "from pyspotstream.data.OPM import get_opm, load_opm\n",
    "\n",
    "from river import tree as river_tree\n",
    "from river import compose as river_compose\n",
    "from river import linear_model as river_linear_model\n",
    "from river import preprocessing as river_preprocessing\n",
    "from river import feature_extraction as river_feature_extraction\n",
    "from river import neural_net as river_nn\n",
    "from river import optim as river_optim\n",
    "from river import stream as river_stream\n",
    "from river import ensemble as river_ensemble\n",
    "from river import drift as river_drift\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "import warnings\n",
    "from sklearn import exceptions\n",
    "warnings.filterwarnings(action='ignore', category=exceptions.ConvergenceWarning)\n",
    "warnings.filterwarnings(action='ignore', category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_opm(overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Load OPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = load_opm(data_type=\"num\", n=10000, sorted=True, verbose=False)\n",
    "y_train = y_train.squeeze()  # necessary because Y is a one column df\n",
    "X_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Generate Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_d = copy.deepcopy(X_train)\n",
    "y_d = copy.deepcopy(y_train)\n",
    "\n",
    "drift = [1.1,10.0,0.1,1.0]\n",
    "\n",
    "mr = generate_drift(X_train,drift) \n",
    "y_d = np.array(y_d).reshape(-1,)\n",
    "X_d[\"Assessed Value\"] = X_d[\"Assessed Value\"] * mr\n",
    "y_d = y_d * mr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Data with drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = range(X_d.shape[0])\n",
    "plt.plot(x_1, y_d, color='grey', marker='o', linestyle='dashed', linewidth=.2, markersize=.1)\n",
    "for i in range(1,4):\n",
    "    plt.axvline(x=i*np.divmod(X_d.shape[0], 4)[0], color='r', linestyle='-', linewidth = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = copy.deepcopy(X_d)\n",
    "y_train = pd.Series(copy.deepcopy(y_d))\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Selection of the metric\n",
    "\n",
    "* For example `MAE` for regression and `Accuracy` for classification. \n",
    "* Should be implemented as an argument of the function that calls the experiments (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = sk_metrics.mean_absolute_error\n",
    "metric_name = metric.__class__.__name__\n",
    "metric_name = \"MAE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Experimente Batch ML\n",
    "\n",
    "* The function `baseline_batch_experiment()` implements the classic batch ML evaluation:\n",
    "  * Die gegebenen N Samples werden in ein Training- und ein Testset unterteilt.\n",
    "  * Das Modell wird einmal auf dem Trainingsdaten trainiert und erstellt dann Vorhersagen auf den Testdaten\n",
    "  * Here: 10 % are used for training and 90 % for testing (as default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Batch Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple example how to call the batch eval with decision trees.\n",
    "* Should be available for other learners as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.eval.evaluation import baseline_batch_experiment\n",
    "\n",
    "X = copy.deepcopy(X_train)\n",
    "Y = copy.deepcopy(y_train)\n",
    "model = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "dtr_time, dtr_mae, dtr_mem, dtr_model = baseline_batch_experiment(X,Y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Batch Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Second simple example how to call the batch eval with random forest.\n",
    "* Should be available for other learners as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.eval.evaluation import baseline_batch_experiment\n",
    "\n",
    "X = copy.deepcopy(X_train)\n",
    "Y = copy.deepcopy(y_train)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "rf_time, rf_mae, rf_mem, rf_model = baseline_batch_experiment(X,Y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Select Default Batch Results: dt or rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = True\n",
    "if DT:\n",
    "    batch_mae = dtr_mae\n",
    "    batch_time = dtr_time\n",
    "    batch_mem = dtr_mem\n",
    "else:\n",
    "    batch_mae = rf_mae\n",
    "    batch_time = rf_time\n",
    "    batch_mem = rf_mem    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Mini-Batch Learning \n",
    "\n",
    "* Synonym: Multiple Instance Learning\n",
    "* Mini-batch learning is implemented via the function `eval_multiple_instances`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lin- und logspace\n",
    "\n",
    "* Standardmäßig wird eine lineare, äquidistante Partitionierung vorgenommen (`linspace`). Alternativ kann auch eine logarithmisch skalierte Partitionierung durchgeführt werden (`lgospace`).\n",
    "\n",
    "### Modelle\n",
    "\n",
    "* Als Batch Lerner werden die Modelle aus `scikit-learn` verwendet:\n",
    "  * Klassifikation:\n",
    "    * LogisticRegression\n",
    "    * DescisionTreeClassifier\n",
    "  * Regression:\n",
    "    * LinearRegression\n",
    "    * SGDRegressor\n",
    "\n",
    "* Aus dem `river` Paket werden die folgenden Modelle verwendet:\n",
    "  * MLPRegressor\n",
    "  * LinearRegression \n",
    "* These perform really poor see `river`  documentation.\n",
    "\n",
    "* Die Variable `m_river` legt fest, wie die Modelle gebaut (gefitted) und zur Vorhersage verwendet werden (predict).\n",
    "* Dies geschieht in `scikit-learn` anders als in `river`:\n",
    "* Die `scikit-learn` Modelle verwenden die Methoden\n",
    "  * `fit(x,y)` und \n",
    "  * `predict(x)`.\n",
    "* Die `river` Modelle verwenden die Methoden\n",
    "  * `learn_many(x,y)` und\n",
    "  * `predict_many(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = sk_pipeline.make_pipeline(sk_preprocessing.StandardScaler(),\n",
    "                                  sk_linear_model.LinearRegression())\n",
    "\n",
    "m_LinearRegression_times, m_LinearRegression_scores, m_LinearRegression_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model=model)\n",
    "m_LinearRegression_runtime = time.time() - start_time\n",
    "print(f\"--- %s seconds ---{m_LinearRegression_runtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = sk_pipeline.make_pipeline(sk_preprocessing.StandardScaler(),\n",
    "                                  sk_linear_model.SGDRegressor(max_iter=1, tol=1e-3))\n",
    "\n",
    "m_SGDRegressor_times, m_SGDRegressor_scores, m_SGDRegressor_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model=model)\n",
    "m_SGDRegressor_runtime = time.time() - start_time\n",
    "print(f\"--- %s seconds ---{m_SGDRegressor_runtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = sk_pipeline.make_pipeline(sk_preprocessing.MinMaxScaler(),\n",
    "                                  sk_linear_model.SGDRegressor(max_iter=1, tol=1e-3))\n",
    "\n",
    "m_SGDRegressor_MinMaxScaler_times, m_SGDRegressor_MinMaxScaler_scores, m_SGDRegressor_MinMaxScaler_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model=model)\n",
    "m_SGDRegressor_MinMaxScaler_runtime = time.time() - start_time\n",
    "print(f\"--- %s seconds ---{m_SGDRegressor_MinMaxScaler_runtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model = sk_pipeline.make_pipeline(sk_preprocessing.StandardScaler(),\n",
    "                                  DecisionTreeRegressor(random_state=0))\n",
    "\n",
    "m_DTRegressor_times, m_DTRegressor_scores, m_DTRegressor_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model=model)\n",
    "m_DTRegressor_runtime = time.time() - start_time\n",
    "print(f\"--- %s seconds ---{m_DTRegressor_runtime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achtung: Der folgende Code benötigt bereits bei einer Sample Größe von 10000 ca. 2 Minuten!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_MINIBATCH = False\n",
    "if RF_MINIBATCH:\n",
    "    start_time = time.time()\n",
    "    m_RFRegressor_name = \"RFRegressor\"\n",
    "    m_RFRegressor_times, m_RFRegressor_scores, m_RFRegressor_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model='RFRegressor', x_part='linspace', metric=metric)\n",
    "    m_RFRegressor_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Batch Lerner aus `river`\n",
    "\n",
    "* Nicht sonderlich geeignet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPR = False\n",
    "if MLPR:\n",
    "    m_MLPR_name = \"MLPR\"\n",
    "    m_MLPR_times, m_MLPR_scores, m_MLPR_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model='MLPR', x_part='linspace', metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_Batch_Regression = False\n",
    "if river_Batch_Regression:\n",
    "    m_river_LinearRegression_name = \"river_LinearRegression\"\n",
    "    m_river_LinearRegression_times, m_river_LinearRegression_scores, m_river_LinearRegression_mem, _ = eval_multiple_instances(X=X_train, y=y_train, model='river_LinearRegression', x_part='linspace', metric=metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 The Plot function \n",
    "\n",
    "* Please implement this as a function of the pyspotstream package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Function Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Calling the plot function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1 Plotting MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.plot.plotmulti import plot_multiple_instances_results\n",
    "\n",
    "algorithm_scores_dict = {\"LinearRegression\":m_LinearRegression_scores,\n",
    "                  \"SGDRegressor\":m_SGDRegressor_scores,\n",
    "                  \"SGDRegressor_MinMaxScaler\":m_SGDRegressor_MinMaxScaler_scores,\n",
    "                  \"DTRegressor\":m_DTRegressor_scores}\n",
    "\n",
    "plot_multiple_instances_results(algorithm_scores_dict, y_label=\"MAE\", default=batch_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Plotting Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_times_dict = {\"LinearRegression\":m_LinearRegression_times,\n",
    "                  \"SGDRegressor\":m_SGDRegressor_times,\n",
    "                  \"SGDRegressor_MinMaxScaler\":m_SGDRegressor_MinMaxScaler_times,\n",
    "                  \"DTRegressor\":m_DTRegressor_times}\n",
    "\n",
    "plot_multiple_instances_results(algorithm_times_dict, y_label=\"Number of Seconds\", default=batch_time, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2 Plotting Mem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm_mem_dict = {\"LinearRegression\":m_LinearRegression_mem,\n",
    "                  \"SGDRegressor\":m_SGDRegressor_mem,\n",
    "                  \"SGDRegressor_MinMaxScaler\":m_SGDRegressor_MinMaxScaler_mem,\n",
    "                  \"DTRegressor\":m_DTRegressor_mem}\n",
    "\n",
    "plot_multiple_instances_results(algorithm_mem_dict, y_label= \"Peak Memory, kB\", default=batch_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Online Machine Learning\n",
    "\n",
    "* Single Instance Learning oder Interleaved test-then-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Die Methode `eval_single_instances()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspotstream.eval.evaluation import eval_single_instances\n",
    "\n",
    "m_river_LinearRegression = river_linear_model.LinearRegression(intercept_lr=.1) \n",
    "model_times, model_scores, model_mem, model = eval_single_instances(X=X_train,\n",
    "                                                                    y=y_train,\n",
    "                                                                    model=m_river_LinearRegression,\n",
    "                                                                    metric=metric,\n",
    "                                                                    task=\"reg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Daten\n",
    "\n",
    "* OPM, Task: Regression, no missing values,  no categorical values, d.h. derselbe Datensatz wie für C.MI.ETM.1 und C.SI.ETM.1\n",
    "* Es wird der komplette Datensatz verwendet, keine explizite Aufteilung in test, validation, training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = California_Housing(data_format='kaggle', binary_clf=False, drop_nan= True, drop_categorical=True)\n",
    "# X = dataset.data.data\n",
    "# y = dataset.data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Preprocessing\n",
    "\n",
    "* Either StandardScaler or MinmaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = river_preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Models\n",
    "\n",
    "* User specify models that can be passed to the evaluation function `eval_single_instances()`\n",
    "* This is much better than `eval_multiple_instances()` from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_river_LinearRegression = river_linear_model.LinearRegression(intercept_lr=.1) \n",
    "\n",
    "m_river_HTR = river_tree.HoeffdingAdaptiveTreeRegressor(grace_period=50, \n",
    "                                                        model_selector_decay=0.3,\n",
    "                                                        seed=0)\n",
    "\n",
    "m_river_RF = river_ensemble.AdaptiveRandomForestRegressor(seed=42)\n",
    "\n",
    "m_river_HATR = river_tree.HoeffdingAdaptiveTreeRegressor(max_depth= 30, \n",
    "                                                         drift_detector= river_drift.EDDM(warm_start = 30), \n",
    "                                                         grace_period=50, \n",
    "                                                         splitter = river_tree.splitter.EBSTSplitter(), \n",
    "                                                         seed=0) \n",
    "\n",
    "m_river_HTR_2 = river_tree.HoeffdingTreeRegressor(max_depth= 30, splitter = river_tree.splitter.EBSTSplitter()) \n",
    "\n",
    "m_river_ARFR = river_ensemble.AdaptiveRandomForestRegressor(max_depth = 20, \n",
    "                                                            splitter = river_tree.splitter.EBSTSplitter(), \n",
    "                                                            drift_detector = river_drift.ADWIN(), \n",
    "                                                            seed=0) \n",
    "\n",
    "#\n",
    "base_model = river_tree.HoeffdingTreeRegressor(grace_period=50)\n",
    "m_river_SRP = model = river_ensemble.SRPRegressor(\n",
    "    model=base_model,\n",
    "    training_method=\"patches\",\n",
    "    n_models=3,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Running the experiments: OML, Single Instance \n",
    "\n",
    "* Aufruf von `eval_single_instance()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_LM = False\n",
    "if RIVER_LM:\n",
    "    m_river_LinearRegression_name = \"m_river_LinearRegression\"\n",
    "    m_river_LinearRegression_times, m_river_LinearRegression_scores, m_river_LinearRegression_mem, m_river_LinearRegression_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_LinearRegression,\n",
    "        metric=metric,\n",
    "        task=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_HTR = False\n",
    "if RIVER_HTR:\n",
    "    start_time = time.time()\n",
    "    m_river_HTR_name = \"m_river_HTR\"\n",
    "    m_river_HTR_times, m_river_HTR_scores, m_river_HTR_mem, m_river_LinearRegression_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_HTR,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_HTR_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_RF = False\n",
    "if RIVER_RF:\n",
    "    start_time = time.time()\n",
    "    m_river_RF_name = \"m_river_Random_ForestRegr\"\n",
    "    m_river_RF_times, m_river_RF_scores, m_river_RF_mem, m_river_RF_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_RF,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_RF_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_SRP = False\n",
    "if RIVER_SRP:\n",
    "    start_time = time.time()\n",
    "    m_river_SRP_name =\"m_river_SRP: Streaming Random Patches ensemble\"\n",
    "    m_river_SRP_times, m_river_SRP_scores, m_river_SRP_mem, m_river_SRP_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_SRP,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_SRP_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_HATR = True\n",
    "if RIVER_HATR:\n",
    "    start_time = time.time()\n",
    "    m_river_HATR_name =\"m_river_HATR\"\n",
    "    m_river_HATR_times, m_river_HATR_scores, m_river_HATR_mem, m_river_HATR_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_HATR,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_HATR_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_HTR_2 = False\n",
    "if RIVER_HTR_2:\n",
    "    start_time = time.time()\n",
    "    m_river_HTR_2_name =\"m_river_HTR_2\"\n",
    "    m_river_HTR_2_times, m_river_HTR_2_scores, m_river_HTR_2_mem, m_river_HTR_2_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_HTR_2,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_HTR_2_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RIVER_ARFR = False\n",
    "if RIVER_ARFR:\n",
    "    start_time = time.time()\n",
    "    m_river_ARFR_name =\"m_river_ARFR\"\n",
    "    m_river_ARFR_times, m_river_ARFR_scores, m_river_ARFR_mem, m_river_ARFR_model = eval_single_instances(\n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        model=m_river_ARFR,\n",
    "        metric=metric,\n",
    "        task=\"reg\")\n",
    "    m_river_ARFR_runtime = time.time() - start_time\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ergebnisdarstellung\n",
    "\n",
    "* Aufruf von `plot_results()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_score_dict = {m_river_HATR_name:m_river_HATR_scores}\n",
    "\n",
    "plot_multiple_instances_results(online_score_dict, y_label=metric_name, default=batch_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_time_dict = {m_river_HATR_name:m_river_HATR_times}\n",
    "\n",
    "plot_multiple_instances_results(online_time_dict, y_label=\"Number of Seconds\", default=batch_time, log_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_mem_dict = {m_river_HATR_name:m_river_HATR_mem}\n",
    "\n",
    "plot_multiple_instances_results(online_mem_dict, y_label= \"Peak Memory, kB\", default=batch_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Global Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = algorithm_scores_dict | online_score_dict # Only runable with 3.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multiple_instances_results(sl, y_label= metric_name, default=batch_time,log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Zeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = algorithm_times_dict | online_time_dict\n",
    "\n",
    "plot_multiple_instances_results(tl, y_label=\"Number of Seconds\", default=batch_time, log_y=True, marker=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Speicher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = algorithm_mem_dict | online_mem_dict\n",
    "\n",
    "plot_multiple_instances_results(ml, y_label=\"Peak Memory, kB\", default=batch_mem, log_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Draw Hoeffding Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_river_HATR.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_river_RF.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sicherung der Ergebnisse mit `dill`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"40_OPM\" + time.strftime(\"%Y%m%d-%H%M%S\") + \".dill\"\n",
    "dill.dump_session(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RELOAD = False\n",
    "#  Dateinamen mit Zeitstempel erweitern:\n",
    "if RELOAD == True:\n",
    "    import dill\n",
    "    dill.load_session(\"33_OPM20221122-173554.dill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "81c77de872def749acd68d9955e19f0df6803301f4c1f66c3444af66334112ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
